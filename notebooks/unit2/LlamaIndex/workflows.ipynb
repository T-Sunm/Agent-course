{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Sunm/Agent-course/blob/main/notebooks/unit2/LlamaIndex/workflows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-8N3jMuLTEw"
      },
      "source": [
        "# Workflows in LlamaIndex\n",
        "\n",
        "\n",
        "This notebook is part of the [Hugging Face Agents Course](https://www.hf.co/learn/agents-course), a free Course from beginner to expert, where you learn to build Agents.\n",
        "\n",
        "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
        "\n",
        "## Let's install the dependencies\n",
        "\n",
        "We will install the dependencies for this unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d6sGFpDhLTEy"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index datasets llama-index-callbacks-arize-phoenix llama-index-vector-stores-chroma llama-index-utils-workflow llama-index-llms-huggingface-api pyvis -U -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2YCrgwBLTEz"
      },
      "source": [
        "And, let's log in to Hugging Face to use serverless Inference APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qKna4VMuLTEz"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "\n",
        "# login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xECRQEG4LTE0"
      },
      "source": [
        "## Basic Workflow Creation\n",
        "\n",
        "We can start by creating a simple workflow. We use the `StartEvent` and `StopEvent` classes to define the start and stop of the workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DEoyBFp8LTE0",
        "outputId": "d0dc94e2-04a9-42bb-e476-fb38b5f1c9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, world!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
        "\n",
        "\n",
        "class MyWorkflow(Workflow):\n",
        "    @step\n",
        "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
        "        # do something here\n",
        "        return StopEvent(result=\"Hello, world!\")\n",
        "\n",
        "\n",
        "w = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "async def main():\n",
        "    w = MyWorkflow(timeout=10, verbose=False)\n",
        "    result = await w.run()\n",
        "    print(result)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "mnZSwog7Lta7",
        "outputId": "8b65898a-2223-44be-f521-d787acc9f9f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(MyWorkflow, filename=\"basic_workflow.html\")\n"
      ],
      "metadata": {
        "id": "a0ohJA0ESfAo",
        "outputId": "394882cf-5b08-4bf2-b5ed-b50e30337310",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic_workflow.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Th0cHULTE0"
      },
      "source": [
        "## Connecting Multiple Steps\n",
        "\n",
        "We can also create multi-step workflows. Here we pass the event information between steps. Note that we can use type hinting to specify the event type and the flow of the workflow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Event\n",
        "# step_one takes a StartEvent and returns a FirstEvent\n",
        "# step_two takes a FirstEvent and returns a SecondEvent\n",
        "# step_three takes a SecondEvent and returns a StopEvent\n",
        "\n",
        "class FirstEvent(Event):\n",
        "    first_output: str\n",
        "\n",
        "\n",
        "class SecondEvent(Event):\n",
        "    second_output: str\n",
        "\n",
        "\n",
        "class MyWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> FirstEvent:\n",
        "        print(ev.first_input)\n",
        "        return FirstEvent(first_output=\"First step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: FirstEvent) -> SecondEvent:\n",
        "        print(ev.first_output)\n",
        "        return SecondEvent(second_output=\"Second step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_three(self, ev: SecondEvent) -> StopEvent:\n",
        "        print(ev.second_output)\n",
        "        return StopEvent(result=\"Workflow complete.\")\n",
        "\n",
        "\n",
        "w = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run(first_input=\"Start the workflow.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "4MaQ47m8WOxl",
        "outputId": "dd00bb14-c445-4b00-b4db-fb2da812ce5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start the workflow.\n",
            "First step complete.\n",
            "Second step complete.\n",
            "Workflow complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BVXnD70rLTE1",
        "outputId": "30566702-8d0e-44ad-bba3-52fe7e1b9738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Finished processing: Step 1 complete'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "\n",
        "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(MyWorkflow, filename=\"multi_step_workflow.html\")"
      ],
      "metadata": {
        "id": "1BAgKkK3XCcF",
        "outputId": "df542e34-7d01-4a67-9796-c5f3d9db00a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi_step_workflow.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMiyVzwALTE1"
      },
      "source": [
        "## Loops and Branches\n",
        "\n",
        "We can also use type hinting to create branches and loops. Note that we can use the `|` operator to specify that the step can return multiple types."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loops"
      ],
      "metadata": {
        "id": "_lV-FFgfmAj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "4QlNH-abLTE1",
        "outputId": "2db99156-4ac4-4043-c90f-685a857011ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good thing happened\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Finished processing: First step complete.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "from llama_index.core.workflow import Event\n",
        "import random\n",
        "\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "\n",
        "class LoopEvent(Event):\n",
        "    loop_output: str\n",
        "\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
        "        if random.randint(0, 1) == 0:\n",
        "            print(\"Bad thing happened\")\n",
        "            return LoopEvent(loop_output=\"Back to step one.\")\n",
        "        else:\n",
        "            print(\"Good thing happened\")\n",
        "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
        "      # Use the intermediate result\n",
        "      final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "      return StopEvent(result=final_result)\n",
        "\n",
        "\n",
        "w = MultiStepWorkflow(verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw_all_possible_flows(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI2q8MnummVo",
        "outputId": "40c5e158-4290-4c54-d89d-119527fdc52b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "workflow_all_flows.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Branch"
      ],
      "metadata": {
        "id": "y-nvvik5mC3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BranchA1Event(Event):\n",
        "    payload: str\n",
        "\n",
        "\n",
        "class BranchA2Event(Event):\n",
        "    payload: str\n",
        "\n",
        "\n",
        "class BranchB1Event(Event):\n",
        "    payload: str\n",
        "\n",
        "\n",
        "class BranchB2Event(Event):\n",
        "    payload: str\n",
        "\n",
        "\n",
        "class BranchWorkflow(Workflow):\n",
        "    @step\n",
        "    async def start(self, ev: StartEvent) -> BranchA1Event | BranchB1Event:\n",
        "        if random.randint(0, 1) == 0:\n",
        "            print(\"Go to branch A\")\n",
        "            return BranchA1Event(payload=\"Branch A\")\n",
        "        else:\n",
        "            print(\"Go to branch B\")\n",
        "            return BranchB1Event(payload=\"Branch B\")\n",
        "\n",
        "    @step\n",
        "    async def step_a1(self, ev: BranchA1Event) -> BranchA2Event:\n",
        "        print(ev.payload)\n",
        "        return BranchA2Event(payload=ev.payload)\n",
        "\n",
        "    @step\n",
        "    async def step_b1(self, ev: BranchB1Event) -> BranchB2Event:\n",
        "        print(ev.payload)\n",
        "        return BranchB2Event(payload=ev.payload)\n",
        "\n",
        "    @step\n",
        "    async def step_a2(self, ev: BranchA2Event) -> StopEvent:\n",
        "        print(ev.payload)\n",
        "        return StopEvent(result=\"Branch A complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_b2(self, ev: BranchB2Event) -> StopEvent:\n",
        "        print(ev.payload)\n",
        "        return StopEvent(result=\"Branch B complete.\")\n",
        "\n",
        "b = BranchWorkflow(verbose=False)\n",
        "result = await b.run()\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "-gDQlI33mFff",
        "outputId": "b2adc4dc-f108-4c10-a610-057dc6649079"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to branch A\n",
            "Branch A\n",
            "Branch A\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Branch A complete.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw_all_possible_flows(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHlpDYpGmX0n",
        "outputId": "73660601-bdbb-4c59-a67a-6a9ae7c2bfa3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "workflow_all_flows.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtnqa_uOLTE2"
      },
      "source": [
        "![drawing](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflow-draw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbX5A4RbLTE2"
      },
      "source": [
        "### State Management\n",
        "\n",
        "Instead of passing the event information between steps, we can use the `Context` type hint to pass information between steps.\n",
        "This might be useful for long running workflows, where you want to store information between steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "29Z7vwJ4LTE2",
        "outputId": "ddad883a-7b1f-43ff-e88f-a0fe1979f404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the capital of France?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Finished processing: Step 1 complete'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "from llama_index.core.workflow import Event, Context\n",
        "from llama_index.core.agent.workflow import ReActAgent\n",
        "\n",
        "\n",
        "class ProcessingEvent(Event):\n",
        "    intermediate_result: str\n",
        "\n",
        "\n",
        "class MultiStepWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent, ctx: Context) -> ProcessingEvent:\n",
        "        # Process initial data\n",
        "        await ctx.set(\"query\", \"What is the capital of France?\")\n",
        "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: ProcessingEvent, ctx: Context) -> StopEvent:\n",
        "        # Use the intermediate result\n",
        "        query = await ctx.get(\"query\")\n",
        "        print(f\"Query: {query}\")\n",
        "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
        "        return StopEvent(result=final_result)\n",
        "\n",
        "\n",
        "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
        "result = await w.run()\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YCvQxiTLTE2"
      },
      "source": [
        "## Multi-Agent Workflows\n",
        "\n",
        "We can also create multi-agent workflows. Here we define two agents, one that multiplies two integers and one that adds two integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1cOSQJPoLTE2",
        "outputId": "693bb8c0-a2bb-44d8-9961-b4cde480ae85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AgentWorkflow' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a33e39efbfa4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Create the workflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m workflow = AgentWorkflow(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0magents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmultiply_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddition_agent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mroot_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multiply_agent\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AgentWorkflow' is not defined"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "# Define some tools\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
        "\n",
        "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
        "multiply_agent = ReActAgent(\n",
        "    name=\"multiply_agent\",\n",
        "    description=\"Is able to multiply two integers\",\n",
        "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
        "    tools=[multiply],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "addition_agent = ReActAgent(\n",
        "    name=\"add_agent\",\n",
        "    description=\"Is able to add two integers\",\n",
        "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
        "    tools=[add],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# Create the workflow\n",
        "workflow = AgentWorkflow(\n",
        "    agents=[multiply_agent, addition_agent],\n",
        "    root_agent=\"multiply_agent\"\n",
        ")\n",
        "\n",
        "# Run the system\n",
        "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SbX5A4RbLTE2"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}